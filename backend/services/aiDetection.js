import OpenAI from 'openai';
import axios from 'axios';
import { splitIntoSentences } from '../utils/textExtractor.js';

const openai = new OpenAI({
  apiKey: process.env.OPENAI_API_KEY,
});

export const detectAIContent = async (text) => {
  try {
    // Run both detection methods in parallel
    const [overallProbability, sentenceHighlights] = await Promise.allSettled([
      getOverallAIProbability(text),
      getSentenceAIHighlights(text)
    ]);

    let probability = 0;
    let highlight = [];

    if (overallProbability.status === 'fulfilled') {
      probability = overallProbability.value;
    } else {
      console.error('Overall AI probability detection failed:', overallProbability.reason);
    }

    if (sentenceHighlights.status === 'fulfilled') {
      highlight = sentenceHighlights.value;
    } else {
      console.error('Sentence AI highlighting failed:', sentenceHighlights.reason);
      // Fallback: create highlights based on overall probability
      const sentences = splitIntoSentences(text);
      highlight = sentences.map(sentence => ({
        text: sentence,
        ai: probability > 0.5
      }));
    }

    return {
      probability,
      highlight
    };

  } catch (error) {
    console.error('AI detection error:', error);
    throw new Error(`AI detection failed: ${error.message}`);
  }
};

// Get overall AI probability using Hugging Face model
const getOverallAIProbability = async (text) => {
  try {
    if (!process.env.HUGGINGFACE_API_KEY) {
      console.warn('Hugging Face API key not configured, using fallback');
      return await getOpenAIFallbackProbability(text);
    }

    const response = await axios.post(
      'https://api-inference.huggingface.co/models/roberta-base-openai-detector',
      { inputs: text },
      {
        headers: {
          'Authorization': `Bearer ${process.env.HUGGINGFACE_API_KEY}`,
          'Content-Type': 'application/json'
        },
        timeout: 30000
      }
    );

    if (response.data && Array.isArray(response.data) && response.data.length > 0) {
      // Find the AI/generated label probability
      const aiLabel = response.data[0].find(item => 
        item.label === 'GENERATED' || item.label === 'AI' || item.label === 'FAKE'
      );
      
      return aiLabel ? aiLabel.score : 0;
    }

    return 0;

  } catch (error) {
    console.error('Hugging Face API error:', error);
    // Fallback to OpenAI-based detection
    return await getOpenAIFallbackProbability(text);
  }
};

// Fallback AI probability detection using OpenAI
const getOpenAIFallbackProbability = async (text) => {
  try {
    if (!process.env.OPENAI_API_KEY) {
      console.warn('OpenAI API key not configured, returning default probability');
      return 0.1; // Default low probability
    }

    const response = await openai.chat.completions.create({
      model: 'gpt-3.5-turbo',
      messages: [
        {
          role: 'system',
          content: 'You are an AI content detector. Analyze the given text and return only a number between 0 and 1 representing the probability that this text was generated by AI. Return only the number, no explanation.'
        },
        {
          role: 'user',
          content: text.substring(0, 3000) // Limit text length for API
        }
      ],
      max_tokens: 10,
      temperature: 0
    });

    const probabilityStr = response.choices[0]?.message?.content?.trim();
    const probability = parseFloat(probabilityStr);
    
    return isNaN(probability) ? 0.1 : Math.max(0, Math.min(1, probability));

  } catch (error) {
    console.error('OpenAI fallback error:', error);
    return 0.1; // Default low probability
  }
};

// Get sentence-level AI highlights using OpenAI
const getSentenceAIHighlights = async (text) => {
  try {
    if (!process.env.OPENAI_API_KEY) {
      console.warn('OpenAI API key not configured for sentence highlighting');
      const sentences = splitIntoSentences(text);
      return sentences.map(sentence => ({ text: sentence, ai: false }));
    }

    const sentences = splitIntoSentences(text);
    const highlights = [];

    // Process sentences in batches to avoid API limits
    const batchSize = 5;
    for (let i = 0; i < sentences.length; i += batchSize) {
      const batch = sentences.slice(i, i + batchSize);
      
      try {
        const response = await openai.chat.completions.create({
          model: 'gpt-3.5-turbo',
          messages: [
            {
              role: 'system',
              content: 'Analyze each sentence and determine if it was likely generated by AI. Return a JSON array with objects containing "text" and "ai" (boolean) fields. Be conservative - only mark as AI if you are confident.'
            },
            {
              role: 'user',
              content: `Analyze these sentences:\n${batch.map((s, idx) => `${idx + 1}. ${s}`).join('\n')}`
            }
          ],
          max_tokens: 1000,
          temperature: 0
        });

        const content = response.choices[0]?.message?.content?.trim();
        
        try {
          const batchResults = JSON.parse(content);
          if (Array.isArray(batchResults)) {
            highlights.push(...batchResults);
          } else {
            // Fallback: mark all as not AI
            highlights.push(...batch.map(sentence => ({ text: sentence, ai: false })));
          }
        } catch (parseError) {
          console.error('Failed to parse OpenAI response:', parseError);
          // Fallback: mark all as not AI
          highlights.push(...batch.map(sentence => ({ text: sentence, ai: false })));
        }

        // Add delay between batches to respect rate limits
        if (i + batchSize < sentences.length) {
          await new Promise(resolve => setTimeout(resolve, 1000));
        }

      } catch (batchError) {
        console.error('Batch processing error:', batchError);
        // Fallback: mark all as not AI
        highlights.push(...batch.map(sentence => ({ text: sentence, ai: false })));
      }
    }

    return highlights;

  } catch (error) {
    console.error('Sentence highlighting error:', error);
    const sentences = splitIntoSentences(text);
    return sentences.map(sentence => ({ text: sentence, ai: false }));
  }
};